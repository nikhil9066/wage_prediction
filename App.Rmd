---
title: "Wage Prediction Analysis"
author: "Nikhil Prema Chandra Rao"
date: "2024-12-11"
output: html_document
---

## Introduction

This project performs wage prediction analysis using various machine learning models on a wage dataset. The goal is to compare the performance of multiple models including Linear Regression, Random Forest, and XGBoost. Hyperparameter tuning and regularization techniques like Lasso and Ridge Regression are also explored.

In this analysis, we will explore a dataset containing wage information. The goal is to understand the relationship between various factors, such as age, education, and year, and the wage of individuals. By analyzing this data, we aim to build a predictive model that can estimate an individual's wage based on these factors.

### About the Data

The data set, `Wage.csv`, consists of information on individuals' wages along with other attributes that might influence their income. The key variables in this data set include:

-   **age**: The age of the individual.
-   **education**: The highest level of education attained by the individual (e.g., High School, Bachelor's Degree, Master's Degree).
-   **year**: The year when the wage was recorded.
-   **wage**: The wage of the individual, which is our target variable.

This data set allows us to explore the impact of age, education, and the year on an individual's wage. We will perform exploratory data analysis (EDA) to understand the relationships between these variables, followed by fitting a linear regression model to predict wages based on these features.

### Objective of the Analysis

The main objectives of this analysis are as follows:

1.  **Data Pre-processing**: Load and clean the data set to ensure that it is ready for analysis.
2.  **Exploratory Data Analysis (EDA)**: Visualize and summarize the data to uncover patterns and trends.
3.  **Modeling**: Fit a linear regression model to predict wages based on age, education, and year.
4.  **Model Evaluation**: Assess the performance of the regression model and interpret the results.

By the end of this analysis, we aim to understand how different factors contribute to wage variations and use this knowledge to make predictions about an individual's wage.

### load_libraries
This function loads a list of necessary tools (called libraries) that help perform various tasks like reading data, creating plots, and building machine learning models.
```{r load-libraries, message=FALSE, warning=FALSE}
load_libraries <- function() {
  libraries <- c(
    "caret", "car", "readr", "dplyr", "ggplot2", "GGally", "gridExtra",
    "grid", "glmnet", "Metrics", "rpart", "rpart.plot", "pROC", "tidyr", "reshape2",
    "randomForest", "DiagrammeR", "xgboost"
  )
  lapply(libraries, function(lib) {
    suppressMessages(suppressWarnings(require(lib, character.only = TRUE)))
  })
}
```

### load_and_prepare_data
This function reads a file containing wage data and prepares it by converting text into categories and cleaning up unnecessary columns. It ensures that the data is ready for analysis.
```{r}
load_and_prepare_data <- function(file_path) {
  wage_data <- read_csv(file_path)
  wage_data <- wage_data %>%
    mutate(across(where(is.character), as.factor)) %>%
    select(where(~ !is.factor(.) || length(levels(.)) > 1))
  return(wage_data)
}
```

### plot_data_visualizations
This function creates several visual charts to help understand the data better. These include a histogram of wages, a boxplot to show how wages vary by education level, a scatterplot of wage versus age, and a correlation plot showing relationships between different variables.
```{r}
# Generate and plot visualizations
plot_data_visualizations <- function(data) {
  # Wage Histogram
  wage_histogram <- ggplot(data, aes(x = wage)) +
    geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
    ggtitle("Distribution of Wage") +
    xlab("Wage") +
    ylab("Frequency") +
    theme_minimal(base_size = 12)
  
  # Wage by Education Boxplot
  wage_education_boxplot <- ggplot(data, aes(x = education, y = wage, fill = education)) +
    geom_boxplot(outlier.color = "red", outlier.shape = 1, notch = TRUE) +
    ggtitle("Wage Distribution by Education Level") +
    xlab("Education Level") +
    ylab("Wage") +
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
  
  # Wage vs Age Scatterplot
  wage_age_scatterplot <- ggplot(data, aes(x = age, y = wage)) +
    geom_point(alpha = 0.6, color = "darkblue") +
    geom_smooth(method = "lm", color = "red", se = FALSE, linetype = "dashed") +
    ggtitle("Wage vs Age") +
    xlab("Age") +
    ylab("Wage") +
    theme_minimal(base_size = 12)
  
  # Correlation Plot
  correlation_plot <- ggpairs(data[, c("year", "age", "logwage", "wage")],
                               lower = list(continuous = wrap("smooth", color = "blue", alpha = 0.3)),
                               diag = list(continuous = wrap("densityDiag", fill = "blue", alpha = 0.3)),
                               upper = list(continuous = wrap("cor", size = 4))) +
    ggtitle("Correlation Plot")
  
  # Convert ggmatrix to a grob
  correlation_grob <- GGally::ggmatrix_gtable(correlation_plot)
  
  # Improved Layout
  grid.arrange(
    ggplotGrob(wage_histogram),               # Top-left: Wage Histogram
    ggplotGrob(wage_education_boxplot),       # Top-right: Wage by Education
    ggplotGrob(wage_age_scatterplot),         # Bottom-left: Wage vs Age
    correlation_grob,                         # Bottom-right: Correlation Plot
    layout_matrix = rbind(
      c(1, 2),  # Wage Histogram and Education Boxplot side-by-side
      c(3, 4)   # Wage vs Age and Correlation Plot side-by-side
    ),
    top = textGrob("Data Visualizations", gp = gpar(fontsize = 18, fontface = "bold"))
  )
}
```

### add_data_visualizations
This function creates additional charts like scatter plots, density plots, and bar charts to visualize the data in different ways, making it easier to spot trends.
```{r}
add_data_visualizations <- function(data) {
  scatter_age_education <- ggplot(data, aes(x = age, y = wage, color = education)) +
    geom_point(alpha = 0.5) +
    labs(title = "Wage vs Age by Education", x = "Age", y = "Wage")
  
  wage_density <- ggplot(data, aes(x = wage, fill = education)) +
    geom_density(alpha = 0.5) +
    labs(title = "Density Plot of Wage by Education", x = "Wage", y = "Density")
  
  education_barplot <- ggplot(data, aes(x = education, fill = education)) +
    geom_bar() +
    labs(title = "Education Level Distribution", x = "Education Level", y = "Count")
  
  grid.arrange(scatter_age_education, wage_density, education_barplot, nrow = 2)
}
```

### fit_multilinear_regression
This function creates a mathematical model (called a regression) that predicts wages based on other factors. It shows how different factors affect the wage and gives insights into the importance of each.
```{r}
# Function for Multilinear Regression
fit_multilinear_regression <- function(data) {
  model <- lm(wage ~ ., data = data)
  print(summary(model))
  par(mfrow = c(2, 2))
  plot(model)
  return(model)
}
```

### fit_random_forest
This function builds a machine learning model called Random Forest, which can predict wages by considering many factors. It also shows which factors are the most important for predictions.
```{r}
# Function to implement Random Forests
fit_random_forest <- function(data, formula) {
  rf_model <- randomForest(formula, data = data, importance = TRUE)
  print(rf_model)
  varImpPlot(rf_model, main = "Variable Importance (Random Forest)")
  plot(rf_model)
  return(rf_model)
}
```

### fit_xgboost
This function uses another machine learning method, XGBoost, to predict wages. Itâ€™s known for its efficiency and accuracy in handling complex data.
```{r}
# Function to implement XGBoost
fit_xgboost <- function(data, formula) {
  x_data <- model.matrix(formula, data)[, -1]  # Remove intercept
  y_data <- data[[as.character(formula[[2]])]]  # Extract response variable
  
  xgb_model <- xgboost(
    data = as.matrix(x_data), label = y_data, max.depth = 3,
    eta = 0.1, nrounds = 100, objective = "reg:squarederror", verbose = 0
  )
  #plot(xgb_model)
  print(xgb_model)
  return(xgb_model)
}
```

### fit_ridge_regression
This function builds a model similar to linear regression but with a twist. It reduces the impact of less important factors to make the model more stable. It also selects the best settings for the model to improve predictions.
```{r}
# Function for Ridge Regression
fit_ridge_regression <- function(data) {
  # Prepare the model matrix (predictors) and response vector
  x <- model.matrix(wage ~ . - 1, data = data)  # Remove the intercept for glmnet
  y <- data$wage
  
  # Fit ridge regression model with a sequence of lambda values
  ridge_model <- glmnet(x, y, alpha = 0)  # alpha = 0 for ridge regression
  
  # Plot the coefficient path for ridge regression
  plot(ridge_model, xvar = "lambda", label = TRUE)
  title("Coefficient Path for Ridge Regression")
  
  # Perform cross-validation to find the optimal lambda
  cv_ridge <- cv.glmnet(x, y, alpha = 0)
  plot(cv_ridge)
  title("Cross-Validation for Selecting Lambda")
  
  # Extract the best lambda value (minimizing cross-validated mean squared error)
  best_lambda <- cv_ridge$lambda.min
  cat("Best lambda: ", best_lambda, "\n")
  
  # Return the coefficients at the best lambda
  return(coef(ridge_model, s = best_lambda))
}
```

### perform_cross_validation
This function tests a model's ability to predict well by checking it on different parts of the data. It helps choose the best settings for the model to perform better in the real world.
```{r}
# Function to compute cross-validation for Lasso and Ridge
perform_cross_validation <- function(data, alpha) {
  x <- model.matrix(wage ~ . - 1, data = data)
  y <- data$wage
  cv_model <- cv.glmnet(x, y, alpha = alpha)
  plot(cv_model)
  title(ifelse(alpha == 1, "Lasso Cross-Validation", "Ridge Cross-Validation"))
  best_lambda <- cv_model$lambda.min
  cat("Best Lambda: ", best_lambda, "\n")
  return(best_lambda)
}
```

### compare_model_metrics
This function compares the performance of different models by calculating errors in their predictions, helping us see which model is the most accurate.
```{r}
# Function to compare model metrics
compare_model_metrics <- function(actuals, predictions_list, model_names) {
  metrics <- data.frame(
    Model = model_names,
    RMSE = sapply(predictions_list, function(pred) rmse(actuals, pred)),
    MAE = sapply(predictions_list, function(pred) mae(actuals, pred))
  )
  print(metrics)
}
```

### generate_summary_report
This function generates a summary report of the model's performance and saves it to a file. It provides an easy way to review how well the models did in the analysis.
```{r}
# Function to generate summary report
generate_summary_report <- function(metrics, filename = "model_summary_report.txt") {
  sink(filename)
  cat("Summary Report of Model Performance\n")
  cat("-----------------------------------\n")
  print(metrics)
  sink()
  cat("Summary report saved to ", filename, "\n")
}
```

### fit_and_plot_regression_tree
This function creates a regression tree, which is a type of model that predicts wages by splitting the data based on different features. It visualizes the decision-making process of the tree, showing how wages vary across different groups of data.
```{r}
# Function to create and visualize a Regression Tree
fit_and_plot_regression_tree <- function(data, formula) {
  # Fit the regression tree model
  tree_model <- rpart(formula, data = data, method = "anova")
  
  # Plot the regression tree showing the mean and percentage of observations
  rpart.plot(tree_model, main = "Regression Tree for Wage Prediction", 
             extra = 101,  # Display the mean of the dependent variable and percentage of observations
             under = TRUE, # Place node numbers underneath the node labels
             faclen = 0)   # Don't abbreviate factor levels
  
  return(tree_model)
}
```

